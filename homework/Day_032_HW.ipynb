{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習時間\n",
    "相信大家目前都能夠初步了解機器學習專案的流程及步驟，今天的作業希望大家能夠看看全球機器學習巨頭們在做的機器學習專案。以 google 為例，下圖是 Google 內部專案使用機器學習的數量，隨著時間進展，現在早已超過 2000 個專案在使用 ML。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://cdn-images-1.medium.com/max/800/1*U_L8qI8RmYS-MOBrYvXhSA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "底下幫同學整理幾間知名企業的 blog 或機器學習網站 (自行搜尋也可)，請挑選一篇文章閱讀並試著回答\n",
    "1. 專案的目標？ (要解決什麼問題）\n",
    "2. 使用的技術是？ (只需知道名稱即可，例如：使用 CNN 卷積神經網路做影像分類)\n",
    "3. 資料來源？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Google AI blog](https://ai.googleblog.com/)\n",
    "- [Facebook Research blog](https://research.fb.com/blog/)\n",
    "- [Apple machine learning journal](https://machinelearning.apple.com/)\n",
    "- [機器之心](https://www.jiqizhixin.com/)\n",
    "- [雷鋒網](http://www.leiphone.com/category/ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "移動相機，移動人：深度學習深度預測方法\n",
    "Moving Camera, Moving People: A Deep Learning Approach to Depth Prediction\n",
    "https://ai.googleblog.com/2019/05/moving-camera-moving-people-deep.html\n",
    "1.人體模型挑戰視頻為移動攝像機和移動人提供深度監控，但我們的目標是使用移動攝像頭處理視頻並移動人。我們需要構建網絡輸入以彌合這一差距。\n",
    "2.深度預測網絡：模型的輸入包括RGB圖像，人類區域的掩模和非人類區域的初始深度，根據輸入Frame與另一個之間的運動視差（光流）計算視頻中的Frame。該模型輸出Frame的完整深度圖。由MVS計算的深度圖提供對訓練的監督。\n",
    "3.透過觀看移動人學習移動人的深度，我們通過應用基於深度學習的方法來解決這一基本挑戰，該方法可以從普通視頻生成深度圖，其中攝像機和主體都可以自由移動。該模型通過從數據中學習人體姿勢和形狀的先驗來避免直接3D三角測量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
